{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "We first install and import necessary packages."
   ],
   "metadata": {
    "id": "Vv-17QH2M1fW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "Vv-17QH2M1fW"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EwUN5g20CFnH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.8/site-packages (1.13.0)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch) (4.4.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\r\n",
      "You should consider upgrading via the '/Users/giovanniaiello/Dropbox/00/RL/Assignments/Assignment 3/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: gym==0.21 in ./venv/lib/python3.8/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./venv/lib/python3.8/site-packages (from gym==0.21) (1.23.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./venv/lib/python3.8/site-packages (from gym==0.21) (2.2.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\r\n",
      "You should consider upgrading via the '/Users/giovanniaiello/Dropbox/00/RL/Assignments/Assignment 3/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: stable-baselines3==1.6.2 in ./venv/lib/python3.8/site-packages (1.6.2)\r\n",
      "Requirement already satisfied: gym==0.21 in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (0.21.0)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (1.23.4)\r\n",
      "Requirement already satisfied: torch>=1.11 in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (1.13.0)\r\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (3.6.2)\r\n",
      "Requirement already satisfied: cloudpickle in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (2.2.0)\r\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in ./venv/lib/python3.8/site-packages (from stable-baselines3==1.6.2) (4.13.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata~=4.13->stable-baselines3==1.6.2) (3.10.0)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch>=1.11->stable-baselines3==1.6.2) (4.4.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (9.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (3.0.9)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (1.4.4)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (4.38.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (2.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (21.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib->stable-baselines3==1.6.2) (1.0.6)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas->stable-baselines3==1.6.2) (2022.6)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.6.2) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\r\n",
      "You should consider upgrading via the '/Users/giovanniaiello/Dropbox/00/RL/Assignments/Assignment 3/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install gym==0.21\n",
    "!pip install stable-baselines3==1.6.2"
   ],
   "id": "EwUN5g20CFnH"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d82aa472",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "d82aa472"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53edcf8d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Acrobot\n",
    "\n",
    "Next, we instantiate the [Acrobot environment](https://www.gymlibrary.dev/environments/classic_control/acrobot/) from OpenAI Gym and gain a quick understanding of its key variables and methods. \n",
    "\n",
    "The Acrobot environment includes a simple robot with two blue links that are connected by two green joints. The joint connecting the two links is actuated, i.e., it can be controlled by the robot by applying torque to the joint. The goal is to swing the free end of the outer link to reach the target height (shown as the black horizontal line) by using robot's actuation.\n",
    "\n",
    "Follow the hyperlinks to learn more about the envionment [Acrobot environment](https://www.gymlibrary.dev/environments/classic_control/acrobot/)  and its [source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py).\n",
    "\n",
    "![](https://www.gymlibrary.dev/_images/acrobot.gif)"
   ],
   "id": "53edcf8d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "667b69a6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")"
   ],
   "id": "667b69a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64a9a5a4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### States\n",
    "\n",
    "OpenAI Gym environments do not explicitly provide state but instead provide observation. In the Acrobot environment, the state and observation are identical. The state of the Acrobot is a 6-dimensional vector, which provides information about the two rotational joint angles of the robot and their angular velocities. As joint angles and velocity are continuous variables, so is the Acrobot state."
   ],
   "id": "64a9a5a4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "289f3d4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state features: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of state features: {env.observation_space.shape[0]}\")"
   ],
   "id": "289f3d4a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90d7bf86",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Actions\n",
    "\n",
    "In contrast to the continuous state, the action space for this robot is discrete. The robot has only three choices:\n",
    "- apply torque of unit 1 in clockwise direction,\n",
    "- apply torque of until 1 in counter-clockwise direction, or\n",
    "- apply no torque."
   ],
   "id": "90d7bf86"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d296baae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions: {env.action_space.n}\")"
   ],
   "id": "d296baae"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdccf5a0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transition and Reward Function\n",
    "\n",
    "The transition and rewards functions for the environment are not explicitly represented as matrices or tensors. Instead the gym API provides access to the `step` method, which takes in as input an `action` and provides (among other things) an `observation` and `reward`. Also notice the `reset` method, which resets the MDP environment.\n",
    "\n",
    "The following snippet describes the use of `step` and `reset` methods."
   ],
   "id": "cdccf5a0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e4699b41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [ 0.9957404   0.092201    0.9999109   0.0133515  -0.01067897 -0.07086682]\n",
      "Action: 1\n",
      "Next state: [ 0.9968618   0.07916107  0.99997747  0.00671444 -0.11764181  0.00343792]\n",
      "Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "state = env.reset() \n",
    "print(f\"State: {state}\")\n",
    "\n",
    "# Select a random action to play\n",
    "action = env.action_space.sample()\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "# Send this action to the environment to receive the next state and reward\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")"
   ],
   "id": "e4699b41"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2406a55b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem 1.1\n",
    "\n",
    "Next, we will implement the general recipe of Q Learning algorithm to compute the optimal policy in the Acrobot environment. "
   ],
   "id": "2406a55b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e1d5cb3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q Network\n",
    "\n",
    "Since, the robot's state space is continuous, we cannot represent the Q value function exactly. Instead, we will approximate the Q value as a neural network. Let us first define this neural network. \n",
    "\n",
    "Many choices exist for the neural network architecture. We will utilize a multi-layer perceptron, with\n",
    "- input as the state of the Acrobot, and\n",
    "- output as a vector of size 3 denoting Q values of each action for the input state."
   ],
   "id": "2e1d5cb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87cd0e5b",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Approximates the Q Function as a Multi-Layer Perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, env, nodes_per_mlp_layer=[128, 64]):\n",
    "        \"\"\"Initialize the Q Function apprixmated as a Multi-Layer Perceptron.\n",
    "        \n",
    "        Args:\n",
    "            env: An OpenAI Gym environment.\n",
    "            nodes_per_mlp_layer: An array of integers. The length of array equals the number\n",
    "                of hidden layers of the Multi-Layer Perceptron. Each element in the array\n",
    "                equals the number of nodes in the corresponding layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "        # input layer\n",
    "        self.input_layer = nn.Linear(NotImplemented, NotImplemented)\n",
    "        \n",
    "        # hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()   \n",
    "        for k in range(len(nodes_per_mlp_layer) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(NotImplemented, NotImplemented))\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(NotImplemented, NotImplemented)\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Implements the forward pass of the Q Network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input to the Q Network.\n",
    "        \n",
    "        Returns:\n",
    "            Output of the Q Network.\n",
    "        \"\"\"\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "        \n",
    "        ######## PUT YOUR CODE HERE ########"
   ],
   "id": "87cd0e5b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dce0836",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Replay Buffer\n",
    "\n",
    "In addition to the Q Network, our algorithm requires a data structure to store the agent's experiences: Replay Buffer. Let us now define this data structure.\n",
    "\n",
    "Similar to Q Network, many data structure choices exist for the Replay Buffer. We will use a simple buffer to store agent's experiences."
   ],
   "id": "8dce0836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfe889a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A buffer to store agent's experiences.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, buffer_size):\n",
    "        \"\"\"Initialize a ring buffer to store agent's experiences.\n",
    "        \n",
    "        Args:\n",
    "            env: An OpenAI Gym environment.\n",
    "            buffer_size: An integer. The total size of the buffer.\n",
    "        \"\"\"\n",
    "        observation_n = env.observation_space.shape[0]\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.observations = np.zeros((self.buffer_size, observation_n), dtype=np.float32)\n",
    "        self.next_observations = np.zeros((self.buffer_size, observation_n), dtype=np.float32)\n",
    "        self.actions = np.zeros((self.buffer_size,), dtype=np.int64)\n",
    "        self.rewards = np.zeros((self.buffer_size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.buffer_size,), dtype=np.float32)\n",
    "\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "    \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Add an experience to the buffer.\n",
    "        \n",
    "        Args:\n",
    "            state: the current environment state\n",
    "            action: the action executed in the state\n",
    "            next_state: the state after executing the action\n",
    "            reward: the reward received after executing the action\n",
    "            done: Boolean denoting whether the task is completed.        \n",
    "        \"\"\"\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a mini-batch of experiences from the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: An integer. The size of the mini-batch.\n",
    "        \n",
    "        Returns:\n",
    "            Randomly sampled experiences from the replay buffer.\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(self.size, size=batch_size)\n",
    "        observations = torch.from_numpy(self.observations[indices])\n",
    "        next_observations = torch.from_numpy(self.next_observations[indices])\n",
    "        actions = torch.from_numpy(self.actions[indices])\n",
    "        rewards = torch.from_numpy(self.rewards[indices])\n",
    "        dones = torch.from_numpy(self.dones[indices])\n",
    "\n",
    "        return observations, actions, next_observations, rewards, dones"
   ],
   "id": "dfe889a8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1299e53",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Exploration schedule\n",
    "\n",
    "Q Learning typically utilizes epsilon-greedy strategy for exploration. \n",
    "\n",
    "In general, it is useful to explore more initially (when the agent is far from the optimal policy) and less later on in the learning process. To implement this, we can implement a schedule for epsilon.\n",
    "\n",
    "Similar to the Q network and replay buffer architectures, several choices exist for determining the epsilon schedule. We will use a simple linear scheduler to decay epsilon as agent gains more experience. "
   ],
   "id": "a1299e53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4dcbca6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_schedule_for_epsilon(initial_value, final_value, duration, timestep):\n",
    "    \"\"\"Implements a linear scheduler for epsilon.\n",
    "    \n",
    "    Args:\n",
    "        initial_value: (float) Initial value of epsilon.\n",
    "        final_value: (float) Final value of epsilon.\n",
    "        duration: (int) Duration over which to decay epsilon from its initial to final value.\n",
    "        timestep: (int) The current time step.\n",
    "    \n",
    "    Returns:\n",
    "        Value of epsilon at the given timestep.\n",
    "    \"\"\"\n",
    "    slope = (final_value - initial_value) / duration\n",
    "    return max(slope * timestep + initial_value, final_value)"
   ],
   "id": "a4dcbca6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72a7ddb5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### General recipe for Q Learning with Function Approximation\n",
    "\n",
    "Now that we have all the building blocks, we will implement the general recipe for Q learning with function approximation. \n",
    "\n",
    "We will use the AgentBase class from Assignment 2 to help us with the implementation."
   ],
   "id": "72a7ddb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82668534",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.policy = self.make_policy()\n",
    "        self.behavior_policy = self.make_behavior_policy()\n",
    "\n",
    "    def make_policy(self):\n",
    "        \"\"\"\n",
    "        Return a policy function that will be used for evaluation. The policy\n",
    "        takes observation as input and return action\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_behavior_policy(self):\n",
    "        \"\"\"\n",
    "        Similar to make_policy, it returns a policy function. But this one used\n",
    "        for interaction with the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_episode(self, episode_policy):\n",
    "        \"\"\"\n",
    "        Generate one episode with the given policy\n",
    "        \"\"\"\n",
    "        episode = []\n",
    "        done = False\n",
    "        obs = self.env.reset()\n",
    "        episode_return = 0\n",
    "        while not done:\n",
    "            action = episode_policy(obs)\n",
    "            next_obs, reward, done, _ = self.env.step(action)\n",
    "            episode.append([obs, action, reward, next_obs, done])\n",
    "            obs = next_obs\n",
    "            episode_return += reward\n",
    "\n",
    "        return (episode, episode_return)\n",
    "\n",
    "    def evaluate(self, num_eval_episodes=1000, plot_title=\"Evaluation\"):\n",
    "        \"\"\"Evaluates the agent.\"\"\"\n",
    "        list_returns = []\n",
    "        list_average_returns = []\n",
    "        average_return = 0\n",
    "        for episode_idx in range(num_eval_episodes):\n",
    "            _, episode_return = self.run_episode(self.policy)\n",
    "            average_return += (1. / (episode_idx+1)) * (episode_return - average_return)\n",
    "            list_returns.append(episode_return)\n",
    "            list_average_returns.append(average_return)\n",
    "\n",
    "        print(f\"Average reward {round(average_return, 3)}\")\n",
    "        plt.plot(list_returns,'^',label=\"Return\")\n",
    "        plt.plot(list_average_returns,'r',label=\"Average Return\")\n",
    "        plt.ylabel('Return')\n",
    "        plt.xlabel('Episode#')\n",
    "        plt.title(plot_title)\n",
    "        plt.legend()\n",
    "        plt.ylim(-501, 0.0)\n",
    "        plt.show()"
   ],
   "id": "82668534"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21591613",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepQLearning(AgentBase):\n",
    "    \"\"\"Implements a Q Learner with function approximation.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        env,\n",
    "        buffer_size = 200000,\n",
    "        batch_size = 64,\n",
    "        initial_epsilon = 1.0,\n",
    "        final_epsilon = 0.01,\n",
    "        epsilon_decay_duration = 100000,\n",
    "        learning_rate = 0.001,\n",
    "        num_gradient_updates = 1,\n",
    "        q_network_update_frequency = 1,\n",
    "        target_network_update_frequency = 200,\n",
    "        learning_starts_at_step = 10000):\n",
    "        \"\"\"Initializes the Agent.\n",
    "        \n",
    "        Args:\n",
    "            env: An OpenAI Gym environment.\n",
    "            buffer_size: (integer) Size of the replay buffer.\n",
    "            batch_size: (integer) Size of the mini batch.\n",
    "            initial_epsilon: (float) Initial value of epsilon.\n",
    "            final_epsilon: (float) Final value of epsilon.\n",
    "            epsilon_decay_duration: (integer) Duration over which to decay epsilon.\n",
    "            learning_rate: (float) Learning rate for Q network update.\n",
    "            num_gradient_updates: (integer) Number of stochastic gradient updates with each minibatch.\n",
    "            q_network_update_frequency: (integer) Steps after which to update Q network.\n",
    "            target_network_update_frequency: (integer) Steps after which to update target network.            \n",
    "            learning_starts_at_step: (integer) Step at which to begin learning. Before this, the \n",
    "                agent explores and collects experiences in its replay buffer.\n",
    "        \"\"\"\n",
    "        super().__init__(env=env)\n",
    "        \n",
    "        self.gamma = 0.99 # Assume a discount factor of 0.99\n",
    "        self.current_step = 0\n",
    "        self.learning_starts_at_step = learning_starts_at_step\n",
    "        self.batch_size = batch_size\n",
    "        self.num_gradient_updates = num_gradient_updates\n",
    "        self.q_network_update_frequency = q_network_update_frequency\n",
    "        self.target_network_update_frequency = target_network_update_frequency\n",
    "\n",
    "        # Create exploration scheduler\n",
    "        self.epsilon_scheduler = lambda current_step: linear_schedule_for_epsilon(\n",
    "            initial_epsilon, final_epsilon, epsilon_decay_duration, current_step)      \n",
    "\n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "        \n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "        self.state = env.reset()\n",
    "        self.list_returns = []\n",
    "        self.list_average_returns = []\n",
    "        self.average_return = 0.\n",
    "        self.list_moving_average_returns = []\n",
    "        self.moving_average_returns_by_step = np.empty([200000])\n",
    "    \n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self.epsilon_scheduler(self.current_step)\n",
    "\n",
    "    def make_policy(self):\n",
    "        def policy_func(observation):\n",
    "            ######## PUT YOUR CODE HERE ########\n",
    "            \n",
    "            ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "        return policy_func\n",
    "\n",
    "    def make_behavior_policy(self):\n",
    "        def policy_func(observation):\n",
    "            ######## PUT YOUR CODE HERE ########\n",
    "            \n",
    "            ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "        return policy_func\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update the agent.\"\"\"                      \n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "        \n",
    "        ######## PUT YOUR CODE HERE ########\n",
    "        \n",
    "    \n",
    "    def train(self, num_train_episodes, make_plot=False):\n",
    "        \n",
    "        for episode_idx in range(num_train_episodes):\n",
    "            # Reset environment before beginning the episode\n",
    "            done = False\n",
    "            self.state = self.env.reset()\n",
    "            episode_return = 0\n",
    "            \n",
    "            # Run the episode and update the policy \n",
    "            while not done:\n",
    "                # First, generate a step with behavior policy\n",
    "                action = self.behavior_policy(self.state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Update the replay buffer\n",
    "                self.replay_buffer.add(self.state, action, next_state, reward, done)\n",
    "            \n",
    "                # Second, update the agent\n",
    "                self.update()\n",
    "                \n",
    "                # Prepare for next step\n",
    "                self.state = next_state\n",
    "                self.current_step += 1\n",
    "                episode_return += reward\n",
    "                if self.current_step < 200000:\n",
    "                    if len(self.list_moving_average_returns) > 0:\n",
    "                      self.moving_average_returns_by_step[self.current_step] = self.list_moving_average_returns[-1]\n",
    "                    else:\n",
    "                      self.moving_average_returns_by_step[self.current_step] = -500.\n",
    "\n",
    "                if self.current_step % 10000 == 0:\n",
    "                    print(f\"Timestep: {self.current_step}, episode reward (moving average, 20 episodes): {round(self.list_moving_average_returns[-1],2)}\")\n",
    "            \n",
    "            # Store the return for evaluation\n",
    "            self.list_returns.append(episode_return)\n",
    "            self.average_return = np.mean(np.asarray(self.list_returns))\n",
    "            self.list_average_returns.append(self.average_return)\n",
    "\n",
    "            if len(self.list_returns) > 20:\n",
    "                self.list_moving_average_returns.append(\n",
    "                  np.mean(np.asarray(self.list_returns[-20:])))\n",
    "            else:\n",
    "                self.list_moving_average_returns.append(self.average_return)\n",
    "\n",
    "        if make_plot:\n",
    "            plt.plot(self.list_returns,'^',label=\"Return\")\n",
    "            plt.plot(self.list_average_returns,'r',label=\"Average Return (all episodes)\")\n",
    "            plt.plot(self.list_moving_average_returns,'b',label=\"Average Return (last 20 episodes)\")\n",
    "            plt.ylabel('Return')\n",
    "            plt.xlabel('Episode#')\n",
    "            plt.title('Performance during training')\n",
    "            plt.ylim(-501, 0.0)\n",
    "            plt.legend()        \n",
    "            plt.show()        "
   ],
   "id": "21591613"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "661014f4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training the Agent\n",
    "\n",
    "Having implemented the agent, now we will train it using the default hyperparameters provided in the class definition and observe its performance."
   ],
   "id": "661014f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "724e8bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "agent_001 = DeepQLearning(env)"
   ],
   "id": "724e8bb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c2cb127",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_001.evaluate(plot_title=\"Performance before training\", num_eval_episodes=100)"
   ],
   "id": "3c2cb127"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06932c10",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_001.train(1000, make_plot=True)"
   ],
   "id": "06932c10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcvssSrlJylN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_001.evaluate(plot_title=\"Performance after training\", num_eval_episodes=100)"
   ],
   "id": "JcvssSrlJylN"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRvLQpb6XKOJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem 1.2\n",
    "\n",
    "Now that we have implemented the general recipe, let us run ablation studies to assess importance of different components of the Deep Q Learning algorithm.\n",
    "\n",
    "First, we will implement and assess the performance of the original Q Learning with naive function approximation, i.e., without the use of replay buffer and target networks. Note that this is a special case of our general recipe and, in general, we do not expect to work in practice. Let us see if it works for the Acrobot environment."
   ],
   "id": "xRvLQpb6XKOJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V4_ZE-4fRv4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OnlineDeepQLearning(DeepQLearning):\n",
    "    \"\"\"Implements Online Q Learner with function approximation.\"\"\"\n",
    "    \n",
    "    ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "    ######## PUT YOUR CODE HERE ########"
   ],
   "id": "_V4_ZE-4fRv4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPWojLBQbmwG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "agent_002 = OnlineDeepQLearning(env)"
   ],
   "id": "xPWojLBQbmwG"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLIV58UtcAC2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_002.train(500, make_plot=True)"
   ],
   "id": "fLIV58UtcAC2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_JhG9ZUds3O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let us bring back one of the components at a time that made the algorithm successful in Problem 1. First, we will bring back only the replay buffer (and not the target network) and observe its effect on the agent's performance."
   ],
   "id": "q_JhG9ZUds3O"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfaEKoe6hkBv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepQLearningWithoutTargetNetwork(DeepQLearning):\n",
    "    \"\"\"Implements a Deep Q Learner without target network.\"\"\"\n",
    "    \n",
    "    ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "    ######## PUT YOUR CODE HERE ########"
   ],
   "id": "cfaEKoe6hkBv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkmpYdDTd-WL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "agent_003 = DeepQLearningWithoutTargetNetwork(env)"
   ],
   "id": "fkmpYdDTd-WL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_kc9HBMGeaHK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_003.train(1000, make_plot=True)"
   ],
   "id": "_kc9HBMGeaHK"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f-kt9K_e-vU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will bring back only the target network (and not the replay buffer) and observe its effect on the agent's performance."
   ],
   "id": "6f-kt9K_e-vU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCKmIA-fh_6N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DeepQLearningWithoutReplayBuffer(DeepQLearning):\n",
    "    \"\"\"Implements a Deep Q Learner without replay buffer.\"\"\"\n",
    "    ######## PUT YOUR CODE HERE ########\n",
    "\n",
    "    ######## PUT YOUR CODE HERE ########"
   ],
   "id": "oCKmIA-fh_6N"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QD788cSIieA6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "agent_004 = DeepQLearningWithoutReplayBuffer(env)"
   ],
   "id": "QD788cSIieA6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CNVjTdPid2G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_004.train(1000, make_plot=True)"
   ],
   "id": "4CNVjTdPid2G"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXK_YMAYi8sx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In a single plot, show the performance of the four variants that you have implemented above. Specifications of the plot\n",
    "- X axis: Number of Timesteps (0 to 100000)\n",
    "- Y axis: 20-episode simple moving average of episodic rewards."
   ],
   "id": "mXK_YMAYi8sx"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQyBCL7HjIs_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "######## PUT YOUR CODE HERE ########\n",
    "\n",
    "######## PUT YOUR CODE HERE ########"
   ],
   "id": "FQyBCL7HjIs_"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbs90XholVCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem 1.3\n",
    "\n",
    "Select any one hyperparameter of the DeepQLearning agent and study its effect on the agent's performance. You should try at least five different values of this hyperparameter. In a single plot, show the performance of the agent for different values of hyperparameter.  Specifications of the plot\n",
    "- X axis: Number of Timesteps (0 to 100000)\n",
    "- Y axis: 20-episode simple moving average of episodic rewards.\n"
   ],
   "id": "qbs90XholVCz"
  },
  {
   "cell_type": "code",
   "source": [
    "######## PUT YOUR CODE HERE ########\n",
    "\n",
    "######## PUT YOUR CODE HERE ########"
   ],
   "metadata": {
    "id": "j9JlG7oqwEha",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "j9JlG7oqwEha",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 1.4\n",
    "\n",
    "While applying RL in real world, you may consider the use of off-the-shelf implementation of RL algorithms. The package [Stable Baselines 3](stable-baselines3.readthedocs.io/) aims to provide a set of reliable implementations of RL algorithms in PyTorch. \n",
    "\n",
    "Familiarize yourself with this package and use its implementation of Deep Q Network to learn the optimal policy for the Acrobot environment."
   ],
   "metadata": {
    "id": "tffNcpDjveBG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "tffNcpDjveBG"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLCEfJRQqhjx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "######## PUT YOUR CODE HERE ########\n",
    "\n",
    "######## PUT YOUR CODE HERE ########"
   ],
   "id": "OLCEfJRQqhjx"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a single plot, compare the performance of your implementation of the algorithm in Problem 1.1. with the one that you implemented above (Problem 1.4). Specifications of the plot\n",
    "- X axis: Number of Timesteps (0 to 100000)\n",
    "- Y axis: 20-episode simple moving average of episodic rewards."
   ],
   "metadata": {
    "id": "oJ8_eWPewRtB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "oJ8_eWPewRtB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFZP_dinqo4M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "######## PUT YOUR CODE HERE ########\n",
    "\n",
    "######## PUT YOUR CODE HERE ########"
   ],
   "id": "vFZP_dinqo4M"
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}